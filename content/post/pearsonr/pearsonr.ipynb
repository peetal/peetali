{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Speeding Up Computation of Pearson Correlation\n",
    "\n",
    "## Main Take-away:\n",
    "The simple but efficient trick to compute Pearson Correlation faster is to \n",
    "1. Scale the data\n",
    "2. Perform matrix algebra to compute the correlation coefficient. <br>\n",
    "\n",
    "This simple trick in principle can make the computation 3-4 times faster and can be useful when dealing with large vectors and/or need to compute r for multiple things in parallel. Here I'm going to demonstrate this trick first and then quickly explain why it is the case. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Demo \n",
    "## Let's generate two random vectors, each has the length of $10^8$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sim_v1 = np.random.rand(100000000,)\n",
    "sim_v2 = np.random.rand(100000000,)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To compute the pearson correlation between the two vectros, we could use the function pearsonr from scipy.stats"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "start = time.time()\n",
    "print(f'The pearson correaltion coefficient is {pearsonr(sim_v1, sim_v2)[0]}')\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end - start}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The pearson correaltion coefficient is 0.00014023607618081493\n",
      "Time elapsed: 2.6575210094451904\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Or we could scale the two vectors first, and then compute the dot product between them"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "sim_v1_scale = zscore(sim_v1)\n",
    "sim_v2_scale = zscore(sim_v2)\n",
    "N = len(sim_v1_scale)\n",
    "start = time.time()\n",
    "print(f'The pearson correaltion coefficient is {np.dot(np.array(sim_v1_scale), np.array(sim_v2_scale))/N}')\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end - start}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The pearson correaltion coefficient is 0.00014023607618081355\n",
      "Time elapsed: 1.0209388732910156\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interestingly, the two approaches to compute pearson r gives exactly the same output, but the second approach is much faster.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Why it is the case"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The typical formula for computing pearson r is $$ r = \\frac{\\sum(x - \\bar{x})(y - \\bar{y})}{\\sqrt{\\sum(x - \\bar{x})^2(y - \\bar{y})^2}}$$\n",
    "### When both vectors are standarized, meaning both vectors center at 0 and have SDs equal to 1. Thus we have $\\bar{x} = 0$; $\\bar{y} = 0$ and $\\frac{\\sum(x - \\bar{x})^2}{N} = 1$; $\\frac{\\sum(y - \\bar{y})^2}{N} = 1$, with N being the length of the vector (assuming ddof = 0)\n",
    "### Now say we have two standarized N vectors $\\tilde{x}$ and $\\tilde{y}$. The pearson correlation between the two vectors can be computed as $$r = \\frac{\\sum(\\tilde{x} - 0)(\\tilde{y} - 0)}{\\sqrt{N^2}} = \\frac{\\sum\\tilde{x}\\tilde{y}}{N} = \\frac{\\tilde{x}^T\\tilde{y}}{N}$$\n",
    "### The time difference between the two approaches should not have a crazy difference as the complexity for both computations were bounded by O(n). However, matrix multiplication can benefit from modern CPUs parallel computing techniques such as SIMD (single instruction, multiple data). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6b7311dbaeb2387b14e4e8f4bffb2b9c06ec540c0e079697fb7f6240ff5d5095"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}