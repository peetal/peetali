{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, zscore\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Main Take-aways**\n",
    "Sklearn svm.SVC(kernel = \"linear\") has the notoriety of being slow. However, by using the \"precomputed kernel\" and compute the gram matrix separately, we could not only save time when training and testing data, but also save system memory when dealing with large matrices.  \n",
    "\n",
    "Here I'm going to show a demo and some basic math to demonstrate this trick. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. Simulate data**\n",
    "Here we have 40 samples and each sample has $10^7$ number of features. \n",
    "We also arbitrarily assign labels to all samples, with half being 0 and the resting being 1.  \n",
    "\n",
    "We will be using the first 25 epcohs as training and the rest 15 as testing sets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# simulate data\n",
    "sim_data_features = [np.random.rand(10000000,) for i in range(40)]\n",
    "sim_data_labels = np.tile([0,1],20)\n",
    "\n",
    "# seperate training and testing\n",
    "training_feature, testing_feature = sim_data_features[0:25], sim_data_features[25:40],\n",
    "training_label, testing_label = sim_data_labels[0:25], sim_data_labels[25:40]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. Train a SVC(kernel = \"linear\")**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# train clf, time it\n",
    "train_start = time.time()\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(training_feature, training_label)\n",
    "train_end = time.time()\n",
    "print(f'svc linear kernel clf training took {train_end - train_start}')\n",
    "\n",
    "# test clf, time it and print result \n",
    "test_start = time.time()\n",
    "pred_lab = clf.predict(testing_feature)\n",
    "test_end = time.time()\n",
    "print(f'svc linear kernel clf testing took {test_end - test_start}')\n",
    "print(f'Predicted labels are {pred_lab}')\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('printing the decision function outputs for all testing samples')\n",
    "print(clf.decision_function(testing_feature))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "svc linear kernel clf training took 16.423278093338013\n",
      "svc linear kernel clf testing took 6.240693092346191\n",
      "Predicted labels are [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "---------------------------------------------\n",
      "printing the decision function outputs for all testing samples\n",
      "[-0.03974077 -0.03935351 -0.03929058 -0.03904115 -0.0405302  -0.03887102\n",
      " -0.04086935 -0.04088076 -0.03897439 -0.03982852 -0.03934111 -0.04194098\n",
      " -0.04004195 -0.0407289  -0.04057421]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **3. Train a SVC with precomputed gram matrix**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# compute gram matrix for training \n",
    "gram_start = time.time()\n",
    "gram_mat = np.matmul(np.array(training_feature), np.array(training_feature).T)\n",
    "gram_end = time.time()\n",
    "print('--------------------Training---------------------')\n",
    "print(f'Computing gram matrix took {gram_end - gram_start}')\n",
    "\n",
    "# train clf with precomputed gram matrix \n",
    "train_start = time.time()\n",
    "clf = SVC(kernel='precomputed')\n",
    "clf.fit(gram_mat, training_label)\n",
    "train_end = time.time()\n",
    "print(f'clf training took : {train_end - train_start}')\n",
    "print(f'Total time for training: {train_end - gram_start}')\n",
    "print('--------------------Testing---------------------')\n",
    "\n",
    "# compute gram matrix for testing \n",
    "gram_start = time.time()\n",
    "gram_test = np.matmul(np.array(testing_feature), np.array(training_feature).T)\n",
    "gram_end = time.time()\n",
    "print(f'Computing testing kernel took {gram_end - gram_start}')\n",
    "\n",
    "# test clf with precomputed gram matrix \n",
    "test_start = time.time()\n",
    "pred_lab = clf.predict(gram_test)\n",
    "test_end = time.time()\n",
    "print(f'svc precomputed kernel clf testing took {test_end - test_start}')\n",
    "print(f'Total time for testing: {test_end - gram_start}')\n",
    "print(f'Predicted labels are {pred_lab}')\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print('printing the decision function outputs for all testing samples')\n",
    "print(clf.decision_function(gram_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------Training---------------------\n",
      "Computing gram matrix took 3.305522918701172\n",
      "clf training took : 0.18439912796020508\n",
      "Total time for training: 3.490262985229492\n",
      "--------------------Testing---------------------\n",
      "Computing testing kernel took 2.019568920135498\n",
      "svc precomputed kernel clf testing took 0.0002570152282714844\n",
      "Total time for testing: 2.0199968814849854\n",
      "Predicted labels are [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "---------------------------------------------\n",
      "printing the decision function outputs for all testing samples\n",
      "[-0.03974077 -0.03935351 -0.03929058 -0.03904115 -0.0405302  -0.03887102\n",
      " -0.04086935 -0.04088076 -0.03897439 -0.03982852 -0.03934111 -0.04194098\n",
      " -0.04004195 -0.0407289  -0.04057421]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **4. Model with precomputed gram matrix is much more efficient**\n",
    "#### 4.1 First let's look at the time it took for model training and testing. \n",
    "\n",
    "- It is clear to see that the two models were exactly the same, outputing the **same predictive labels**, with the **exact same decision function outputs** (with possible tidy difference). This suggests that the two classifiers share the **same weight matrix that defines the hyperplane**. Given that the two models are identical, the second approach is also much faster **(training time: 16.42s vs. 3.50s; testing time: 6.24s vs. 2.02s)**. \n",
    "\n",
    "#### 4.2 Second, let's look at the inputs to the classifiers:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "print('--------------------Training---------------------')\n",
    "print(f'The shape of the each training sample is {training_feature[0].shape}')\n",
    "print(f'The shape of the training gram matrix is {gram_mat.shape}')\n",
    "print('--------------------Testing---------------------')\n",
    "print(f'The shape of the each training sample is {testing_feature[0].shape}')\n",
    "print(f'The shape of the training gram (kernel) matrix is {gram_test.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------Training---------------------\n",
      "The shape of the each training sample is (10000000,)\n",
      "The shape of the training gram matrix is (25, 25)\n",
      "--------------------Testing---------------------\n",
      "The shape of the each training sample is (10000000,)\n",
      "The shape of the training gram (kernel) matrix is (15, 25)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- As shown above, for the first model, during training, we need to input 25 $10^7$ vectors to the model. On the other hand, we only need to input a 25 x 25 matrix if we use a precomputed kernel. When it comes to testing, again, we do not need to deal with the crazy large vectors. Instead, only a 15 x 25 kernal matrix would do the work here. \n",
    "- As a result, not only does the precomputed kernel save us computation time, it should in principle be much more friendly with memory. \n",
    "- It is also worth noting that, it could be time/memory consuming when computing the gram (kernel) matrix, but such step could be further optimized. Thus, using svm.SVC(kernel = \"precomputed\") should have a better potential then svm.linearSVC. Details can be found on the sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **5. Conceptually how gram matrix does the work**\n",
    "\n",
    "### 5.1 Primary and dual ridge regression \n",
    "In ridge regression, we use regularized least square to find the weight matrix such that we want to minimize, with $\\lambda$ penalizes the size of th weight matrix.\n",
    "$$ \\frac{1}{2}(Xw - y)^T(Xw - y) + \\lambda||w||^2 $$\n",
    "Taking the derivative of the loss with respect to the weight parameter gives us\n",
    "$$ X^TXw + \\lambda w - X^Ty = 0 $$\n",
    "$$ (X^TX + \\lambda I)w = X^Ty $$\n",
    "Because $(X^TX + \\lambda I)w$ is always invertable if $\\lambda$ > 0, we have \n",
    "$$ w = (X^TX + \\lambda I)^{-1}yX^T$$\n",
    "Thus we show here that the weight matrix $w$ can be expressed as a linear combination of a vector $\\alpha$ and $X$ \n",
    "$$ w = \\sum_{i=1}^{n} \\alpha_{i}x_{i} $$\n",
    "$$ \\alpha = (X^TX + \\lambda I)^{-1}y = (G + \\lambda I)^{-1}y $$\n",
    "\n",
    "### 5.2 Think about the dual optimization problem in SVC\n",
    "Indeed the equation is much more complex when it comes to SVC. However, from 5.1, intuitively we can see that the solution of $\\alpha$ can be written as a function of the Gram matrix G, which is $X^TX$. I believe the time complexity of sklearn implementation of $svm.svc(kernel = \"linear\")$ would be $O(l^2n)$ with l being the number of sample and n being the number of features becuase svm.SVC allows more complicated kernels, while linear kernel could be an special case. With precomputed gram matrix, the time complexity now is bounded by O(l^3). Because $l \\ll n$, the precomputed linear kernel is much faster. <br>\n",
    "\n",
    "As my understanding in math grows (casue now obviously i suck), I will update this post with more concrete equations and interpretations. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6b7311dbaeb2387b14e4e8f4bffb2b9c06ec540c0e079697fb7f6240ff5d5095"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}